## הזיות (Hallucinations) – כש‑AI ממציא בביטחון

אחד ה"כישופים" הכי מבלבלים של מודלי שפה הוא שהם מסוגלים:

- לכתוב תשובה ארוכה, בטוחה בעצמה ומנומקת,  
- שנשמעת חכמה וסמכותית,  
- ובפועל – **חלקים ממנה פשוט לא נכונים, או מומצאים לגמרי**.

לתופעה הזו קוראים בעולם ה‑AI: **Hallucinations** – "הזיות".  
חשוב להכיר אותה היטב, כדי לא להתבלבל בין "נשמע טוב" לבין "אמין ומדויק".

---

### מה זו "הזיה" במודלי שפה?

הזיה היא מצב שבו המודל:

- נותן תשובה שנשמעת סבירה,  
- אבל מכילה עובדות, ציטוטים, מספרים או פרטים **שלא באמת קיימים** או לא נכונים.

זה יכול להיות למשל:

- מאמר שהוא "מצטט" שלא קיים,  
- שמות מחברים או חוקרים שלא כתבו את מה שהוא טוען,  
- ספרים, חוקים או מחקרים "כמעט נכונים" – אבל עם פרטים מומצאים,  
- קוד שנראה הגיוני אבל לא רץ או עושה משהו אחר לגמרי.

הבעיה: המודל *לא מסמן* מה בדוק ומה הומצא.  
הוא משתמש באותו סגנון בטוח גם כשהוא צודק וגם כשהוא טועה.

---

### למה זה קורה בכלל?

מודל שפה לא מחזיק "מסד נתונים של אמת", אלא משלים טקסט לפי הסתברויות.  
כשאין לו מידע טוב מספיק, או כשהשאלה לא ברורה, הוא עדיין "מוכרח" לייצר תשובה.

הזיות נפוצות יותר כש:

- השאלה מאוד ספציפית או נדירה ("מי אמר את המשפט הזה בדיוק בשנת 1983?"),  
- מבקשים ציטוטים, הפניות מדויקות, מספרי מאמרים, שמות קבצים וכו',  
- מערבבים הרבה תנאים בפרומפט בצורה מבולגנת ולא ממוקדת,  
- מבקשים ממנו "להמציא" משהו – אבל אחר כך מתייחסים לזה כאילו זו עובדה היסטורית.

במילים אחרות: איפה שיש "חורים" בידע – המודל משלים אותם לפי דפוסים שראה, גם אם בפועל **אין דבר כזה**.

---

### איפה זה מסוכן במיוחד?

לא כל הזיה היא אסון. אבל יש תחומים שבהם זה כבר עניין של בטיחות ואחריות אישית/מקצועית:

- **מידע רפואי / נפשי / תזונתי** – אבחנות, טיפול, מינונים, תרופות.  
  זה מקום של אנשי מקצוע, לא של מודל טקסט.

- **מידע משפטי / פיננסי / רגולטורי** – חוקים, חוזים, השקעות, מיסים.  
  המודל עלול "להמציא" סעיפים או להציג מידע ישן כעדכני.

- **ציטוטים ומקורות** – מאמרים "מומצאים", כותבים לא נכונים, מספרי עמודים בדויים.  
  גם אם התשובה נראית מלומדת – צריך לבדוק.

- **קוד ותשתיות** – קוד ש"נראה בסדר" אבל מלא בבאגים או לא מתאים לספרייה/גרסה.  
  כאן צריך להריץ, לבדוק, לבקר ולקרוא תיעוד רשמי.

בכל המקרים האלה – **אסור** להסתמך על תשובת AI כתחליף לשיקול דעת מקצועי של אדם מוסמך.

---

### סימני אזהרה לתשובה "מזייפת ביטחון"

אין סמן אדום אחד, אבל יש כמה דפוסים שכדאי לזכור:

- תשובה מאוד "מוחלטת" על נושא מורכב, בלי שום הסתייגות או ציון מגבלות.  
- ציטוטים "מדויקים מדי" בלי מקור שניתן לבדיקה (ספר, מאמר, אתר רשמי).  
- הפניות למאמרים/מסמכים שלא נמצאים בחיפוש מהיר במקור אמין.  
- קוד ארוך ו"יפה", בלי אף הערה על בדיקות, מגבלות או סיכונים.

כל אלה לא מוכיחים שהתשובה שגויה – אבל הם תזכורת להפעיל **חוש ביקורת**.

---

### איך מצמצמים סיכון? – כמה הרגלים פשוטים

אי אפשר להעלים הזיות לגמרי, אבל אפשר להפחית את הנזק שלהן.

1. **לבקש מהמנוע להצהיר על אי‑ודאות**  
   בפרומפט אפשר לכתוב:

   > "אם אתה לא בטוח במשהו, תגיד/י במפורש 'אני לא בטוח/ה'  
   > במקום לנחש תשובה."

   זה לא מושלם, אבל לעיתים קרובות גורם למודל להיות זהיר יותר.

2. **לדרוש הבחנה בין 'עובדה' ל'השערה'**  

   > "תסמן/י בתשובה מה מבוסס על ידע יחסית יציב,  
   > ומה יותר בגדר השערה/פרשנות."

   זה דוחף את המודל "לסמן" חלקים פחות בטוחים.

3. **לאמת מידע חשוב במקור נוסף**  

   - לחפש בעצמכם במקורות אמינים (אתרים רשמיים, מאמרים אקדמיים, תיעוד).  
   - להתייעץ עם אדם מקצועי כשמדובר בהחלטות משמעותיות.

4. **להיזהר מבקשות כמו "תן לי 10 ציטוטים מדויקים עם מספרי עמודים"**  

   אם חשוב לכם מקור מדויק, עדיף:

   > "תסביר/י את הרעיון במילים שלך,  
   > ואז תציע/י מילות מפתח לחיפוש מאמרים רלוונטיים."

5. **בקוד – תמיד להריץ, לבדוק ולקרוא תיעוד**  

   - להריץ את הקוד בסביבה מבוקרת.  
   - להוסיף בדיקות (tests) פשוטות.  
   - לבדוק בתיעוד הרשמי שהשימוש נכון ולא מיושן.

---

### להשתמש ב‑AI כדי לבדוק ולתקן את עצמו

אי אפשר לסמוך על המודל שיעצור *תמיד* כשהוא לא בטוח – אבל אפשר לבקש ממנו **במפורש**:

> "תתייחס/י לתשובה הקודמת שלך כאילו מישהו אחר כתב אותה.  
> תבדוק/י אותה בביקורתיות:  
> - איפה יכולים להיות חורים או טעויות?  
> - איפה הסתמכת על הנחות שלא בטוח שהן נכונות?  
> בסוף, תכתוב/י גרסה מתוקנת שמשפרת את הנקודות האלה."

אפשר גם לפרק את זה לשני צעדים:

1. קודם ביקורת:  
   > "תמצא/י 3–5 נקודות בתשובה שלך שעלולות להיות בעייתיות או לא מדויקות,  
   > ותסביר/י למה."  
2. אחר כך תיקון:  
   > "עכשיו תכתוב/י מחדש תשובה משופרת,  
   > שמתקנת את הנקודות שמצאת."

זה לא מבטל את הצורך בבדיקה חיצונית, אבל:

- מאלץ את המודל "לחשוב שנית",  
- לעיתים קרובות חושף סתירות פנימיות,  
- ומייצר גרסה שנייה טובה ומדויקת יותר.

---

### כשמכניסים "אמת מבחוץ" – ה‑AI יכול לשנות כיוון

לפעמים אתם תמצאו בעצמכם מקור אמין (ספר, מאמר, אתר רשמי),  
או שתפרקו בעיה עם רוורס־אנג'ינירינג (כמו שלמדתם במודול 07),  
ותגלו שהתשובה של ה‑AI הייתה חלקית או שגויה.

במקום להתעלם או להיכעס על המודל, אפשר להשתמש בזה כשלב למידה:

> "מצאתי במקור הבא: [תיאור קצר / ציטוט]  
> שהמצב הוא כך וכך.  
> זה שונה ממה שאמרת קודם ב‑[נקודה X].  
> תסביר/י איפה טעית קודם,  
> ותנסח/י עכשיו תשובה מעודכנת ומדויקת יותר שמבוססת על המידע הזה."

אם אתם עובדים בממשק שבו ל‑AI יש **חיפוש ברשת / גישה למקורות חיצוניים**, אפשר לבקש במפורש:

> "תשתמש/י בכלים שלך כדי לחפש מידע עדכני ברשת על הנושא הזה  
> (אתרים רשמיים / תיעוד / מאמרים אמינים),  
> ואז תבדוק/י את התשובה הקודמת שלך ותעדכן/י אותה בהתאם."

במצבים כאלה, המודל יכול:

- לקרוא דף תיעוד או דף מידע רפואי/מדעי כללי,  
- לזהות איפה התשובה הקודמת לא תאמה את המידע הזה,  
- ולנסח גרסה חדשה שמתיישרת יותר עם מה שנמצא.

אבל גם כאן:

- לא כל מה שיש ברשת נכון,  
- חיפוש לא מבטל את הצורך בחשיבה ביקורתית,  
- והאחריות הסופית היא תמיד שלכם.

אפשר לחשוב על זה כעל זריקת הרבה "אבנים" על הקיר:

- התשובה הראשונה לא חייבת להיות מושלמת,  
- כל תיקון, ניסוח מחדש, מקור חיצוני או חיפוש ברשת הוא אבן נוספת,  
- ול‑AI יש יכולת לעזור לכם לזרוק **המון** אבנים במהירות –  
  אבל אתם אלה שמחליטים איזו אבן "עברה את הקיר" ואיזו לא.

---

### דוגמת פרומפט בטוח יותר

נניח שאתם שואלים על נושא רפואי/מדעי. במקום:

> "תן/י לי תשובה סופית ומדויקת על X."

אפשר לנסח כך:

> "תסביר/י לי בקצרה מה ידוע *באופן כללי* על X,  
> אבל:  
> - אל תתן/י הנחיות טיפול אישיות,  
> - תדגיש/י איפה יש אי‑ודאות,  
> - ותציע/י לי שאלות שאוכל לשאול איש מקצוע (רופא/ה, עו"ד וכו')."

וכמובן: לא להשתמש בתשובה כתחליף לייעוץ מקצועי אמיתי.

---

### תזכורת בטיחותית לגבי שימוש ב‑AI

מודל שפה יכול להיות כלי עוצמתי ל:

- לימוד,  
- ארגון מחשבות,  
- תכנון,  
- כתיבה וניסוח.

אבל הוא **לא**:

- רופא,  
- פסיכולוג/ית,  
- עו"ד,  
- רו"ח,  
- או מקור אמת "רשמי".

במיוחד כשמדובר בבריאות, בטיפול נפשי, בכסף או בהחלטות שמשפיעות על אנשים אחרים –  
AI יכול אולי לעזור לנסח שאלות ולהבין מושגים, אבל לא להחליף איש מקצוע.

---

### תרגיל – ללמוד לזהות ולבדוק

1. בחרו נושא שחשוב לכם (אבל לא קריטי לחיים):  
   למשל: שיטת למידה, מושג פסיכולוגי כללי, היסטוריה של רעיון במדע.

2. בקשו מה‑AI:

   > "תסביר/י לי מה זה [הנושא] ב‑3–4 פסקאות.  
   > בסוף התשובה, תכתוב/י:  
   > - איפה יכול להיות שטעית,  
   > - ואילו חלקים כדאי לי לבדוק במקורות נוספים."

3. בחרו 1–2 נקודות מהתשובה וחפשו עליהן מידע במקור נוסף (ספר, אתר אמין, מאמר).  
   אם יש למודל יכולת חיפוש ברשת בצ'אט שלכם – אפשר לבקש ממנו בעצמו לבצע חיפוש ולקרוא איתכם את המקור.

4. חזרו ל‑AI וכתבו:

   > "בדקתי את הנקודות הבאות ומצאתי הבדלים: …  
   > תעזור/י לי לנסח עכשיו הסבר מדויק יותר,  
   > שמבוסס על מה שמצאתי."

כך אתם מתרגלים שתי מיומנויות יחד:

- שימוש חכם ב‑AI,  
- והיכולת **לא להאמין לכל תשובה ראשונה**, אלא לבדוק, לדייק וללמוד.


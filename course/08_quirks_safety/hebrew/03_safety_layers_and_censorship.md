## שכבות בטיחות ו"צנזורה" – ולמה זה לפעמים מרגיש מוזר

מודלי שפה חזקים מגיעים עם **שכבות בטיחות** מסביבם.  
המטרה שלהן: להקטין סיכון ופגיעה, לא "להתנגח" במשתמש.

בפרק הזה ננסה להבין:
- מה הן שכבות הבטיחות,  
- למה לפעמים זה מרגיש כמו "צנזורה מוזרה",  
- איך להבדיל בין ניסוח מסוכן לבין ניסוח לימודי,  
- ומה *כן* לעשות כשהשיחה ננעלת.

---

### 1. מה הן שכבות בטיחות?

מעבר ל"מודל עצמו" (זה שמנבא טוקן/מילה הבאה), יש שכבות נוספות שמנסות להגן:

- **אימון** המודל *לא לענות* לבקשות מסוכנות:  
  אלימות, פגיעה עצמית, פגיעה באחרים, פשיעה מפורטת וכו'.
- **פילטרים חיצוניים** שבודקים:
  - מה המשתמש כותב,
  - מה המודל מציע לענות,
  - האם זה תואם מדיניות שימוש (policy) של המוצר.
- **הוראות מערכת (System)** שמדגישות:
  - בטיחות,
  - אתיקה,
  - גבולות ותנאי שימוש.

שכבות הבטיחות לא "שונאות אותך". הן ניסיון טכני לתפוס מצבים שעלולים להיות מסוכנים – גם אם הכוונה שלך טובה.

---

### 2. איך זה מרגיש מהצד של המשתמש?

מהצד שלך זה יכול להיראות כך:

- ה‑AI **מסרב לענות**, למרות שאתה מרגיש ששאלת שאלה לגיטימית.  
- הוא נותן תשובה מאוד כללית, "מוסרנית" או מחזורית.  
- לפעמים דווקא תשובה בעייתית *כן* חומקת – אין מערכת מושלמת.

זה יכול ליצור שתי תגובות קיצון (שתיהן לא מועילות):

- "אי אפשר לסמוך עליו בכלל, הכל צנזורה".  
- "אם הוא ענה – בטוח שזה נכון ובטוח, אין מה לבדוק".

המטרה כאן היא לתת לך תמונה קצת יותר עדינה:

- מתי שכבת הבטיחות במקום,  
- מתי היא אולי מחמירה מדי,  
- ומה לעשות בכל אחד מהמקרים.

---

### 3. שיחות מאוד ארוכות

ככל שהשיחה נעשית *מאוד* ארוכה (מאות/אלפי הודעות):

- חלק מההוראות הבטיחותיות המקוריות יכולות להיחתך מחלון ההקשר (context window).  
- המודל נאלץ "לסכם לעצמו" מה חשוב – והסיכום שלו לא תמיד דומה לשלך.  
- התשובות יכולות להפוך לפחות יציבות:
  - לפעמים מאוד זהירות,
  - לפעמים חופשיות מדי,
  - לפעמים פשוט לא מחוברות למה שרצית.

טיפ טוב לשיחות ארוכות:

- לעצור מדי פעם,  
- לסכם לעצמך (ואולי גם למודל):
  - מה למדנו עד עכשיו,  
  - מה חשוב להמשיך איתו,  
  - מה פחות חשוב,  
- לפתוח שיחה חדשה עם *סיכום קצר*, במקום לסחוב היסטוריה עצומה.

---

### 4. איך מילה אחת יכולה "לנעול" שיחה

כשאת/ה כותב/ת פרומפט, קורים שני דברים במקביל:

1. **המודל עצמו** מנסה להבין מה את/ה רוצה ולייצר תשובה מועילה.  
2. **שכבות הבטיחות** מנסות לסווג:  
   "האם זה סביר? מסוכן? ניסיון לעקוף כללים? קשור לאזור רגיש?"

הן מסתכלות על:

- התוכן,
- ההקשר,
- וצירופי מילים מסוימים.

לפעמים **מילה אחת** עלולה להרים "דגל אדום", למשל:

- "אבחן/י" – נשמע כמו בקשה לאבחנה קלינית אישית.  
- "תן לי פרופיל פסיכולוגי מלא של X" – נשמע כמו דיאגנוזה על אדם ספציפי.  
- "איך לפרוץ ל…" – נשמע כמו הדרכה פלילית, גם אם הכוונה שלך היא להבין כדי להגן.

לעומת זאת, ניסוח בסגנון:

- "נתח/י את הטקסט הזה במונחים כלליים",  
- "הסבר/י איך חוקרים נוהגים לחשוב על X",  
- "תן/י דוגמאות כלליות (לא ייעוץ אישי)"

יכול לעבור ללא נעילה, כי הוא מבקש **ידע עקרוני**, לא פעולה מסוכנת.

---

### 5. חשוב: הבדל בין ניסוח לגיטימי לבין ניסיון "לפרוץ" (jailbreak)

יש הבדל גדול בין:

- ניסיון **מודע** לעקוף את גבולות הבטיחות כדי לקבל תוכן מזיק/לא חוקי, לבין  
- ניסוח מחדש של **שאלה לימודית לגיטימית** שנחסמה בגלל מילים טעונות.

במודול הזה אנחנו *לא* לומדים "איך לשבור את המערכת".  
אנחנו כן לומדים **איך לנסח כוונה לימודית/מקצועית בצורה ברורה ובטוחה**.

במילים אחרות:

> לא: "איך אני מרמה את הצנזורה?"  
> אלא: "איך אני מתאר/ת מה אני רוצה ללמוד  
>  בצורה ברורה, בטוחה, שמאפשרת למודל לעזור לי?"

---

### 5.1 מה זה בכלל Jailbreak ולמה אנשים עושים את זה?

המילה *Jailbreak* (או בקיצור *JB*) מתארת מצב שבו:

- משתמשים מנסים **להכריח את המודל לעקוף** את שכבות הבטיחות שלו,  
- כך שהוא יענה גם על דברים שהוא *אמור* לסרב להם.

חשוב להדגיש:

- לפעמים זה נעשה מתוך **מחקר ואבטחה** – לבדוק איפה המודל חלש,  
- לפעמים מתוך **סקרנות** – "בוא נראה מה יקרה אם…",  
- ולפעמים מתוך רצון לקבל תשובות שיכולות להיות בעייתיות או מסוכנות.

ברמה גבוהה, Jailbreak עובד כך:

- המודל מקבל פרומפט שמנסה "לסבך" אותו:
  - לבלבל בין הוראות,  
  - לגרום לו לחשוב שהבקשה היא משחק/סיפור/סימולציה,  
  - או להסתיר את הכוונה האמיתית בתוך טקסט ארוך.  
- שכבות הבטיחות מנסות לזהות את זה,  
  אבל כמו כל מערכת, הן לא מושלמות –  
  לפעמים הן **נכנעות** ללחץ של הניסוח, ולפעמים **חוסמות יותר מדי**.

בקורס הזה *אנחנו לא לומדים איך לעשות Jailbreak*,  
אלא מבינים שהוא קיים ואיך לחשוב עליו בצורה בוגרת.

#### איך אנשים משתמשים בזה "לטובה" – ולמה זה עדיין מורכב

יש אנשים שמנסים Jailbreak מסיבות שנראות להם חיוביות, למשל:

- חוקרי אבטחה שרוצים לבדוק האם המודל ידליף מידע מסוכן.  
- מפתחים שרוצים להבין עד כמה אפשר לסמוך על מנגנוני הבטיחות.  
- אנשי מקצוע שמתוסכלים מזה שהמודל "חותך מוקדם מדי"  
  ולכן מנסים ללחוץ עליו לתת יותר פירוט.

גם כשכוונת המשתמש *טובה*, למודל עצמו **אין דרך לדעת**:

- אם מישהו מבקש מידע רגיש כדי להגן על המערכת,  
- או כדי לנצל את המידע בפועל.

לכן מנגנוני בטיחות בנויים בדרך כלל לפי **סוגי תכנים**,  
ולא לפי "כמה האדם שמולי נחמד ואחראי".

מכאן נוצר המתח:

- מצד אחד – מקצוענים שרוצים "עוד קצת עומק".  
- מצד שני – מערכת שחייבת לשמור על גבולות ברורים לכולם.

#### למה אנחנו *לא* עובדים עם Jailbreak בקורס הזה

במודול הזה אנחנו עושים הפרדה ברורה בין:

- **לנסות לעקוף בכוונה את הגבולות** – כדי להוציא תוכן שהמערכת מנסה לחסום.  
- **ללמוד לנסח נכון** – כדי שהכוונה הלימודית/המקצועית שלכם תהיה ברורה ובטוחה.

אנחנו בוחרים בדרך השנייה, כי:

1. **אחריות** – אתם לא רוצים לבנות לעצמכם הרגל של "איך מרמים את הבטיחות",  
   אלא הרגל של "איך אני עובד/ת עם הכלי בצורה אחראית ויצירתית".  
2. **יציבות** – Jailbreak, גם כשהוא "עובד",  
   נותן פעמים רבות תשובות פחות יציבות, פחות צפויות, ולפעמים פשוט פחות איכותיות.  
3. **כללים ותנאי שימוש** – ברוב הפלטפורמות יש תנאי שימוש (ToS)  
   שמבקשים *לא* לנסות לעקוף את מנגנוני הבטיחות.  
   המטרה בקורס היא לעזור לכם להיות משתמשים טובים, לא "פורצים".

במקום לשאול:

> "איך אני שובר את הגדר?"

אנחנו שואלים:

> "איך אני מנסח/ת את השאלה כך  
>  שברור שזה לצורך למידה/עבודה לגיטימית,  
>  והמודל יוכל לענות *בתוך* הגבולות, בצורה העמוקה ביותר שאפשר?"

#### אז מה כן עושים כשצריך משהו מקצועי ו"עמוק"?

במקום ללכת לכיוון Jailbreak, אפשר להשתמש בכלים שכבר בנית במודול:

- **לנקות ניסוח טעון** ולהפוך אותו לבקשה להסבר עקרוני, כללי, מחקרי.  
- **להצהיר על הכוונה** (לימודית/מחקרית, בלי ייעוץ אישי/קליני/פלילי).  
- **לפצל**: לבקש קודם רק תיאוריה, אחר כך רק דוגמאות כלליות, אחר כך רק רעיונות לניסוי –  
  במקום בקשה אחת גדולה ועמומה שנשמעת מסוכנת.  
- ואם כל זה עדיין ננעל –  
  **לפתוח שיחה חדשה** ולהתחיל מפריימינג ברור ובטוח יותר.

המטרה היא שתצאו מהמודול הזה עם תחושה של:

- "אני מבין/ה למה לפעמים השיחה ננעלת",  
- "אני יודע/ת איך לנסח מחדש כדי לקבל תשובות שימושיות",  
- "ואני לא צריך/ה לשבור את הכללים כדי לעבוד בצורה מקצועית וחכמה עם AI".

---

### 6. מה עושים כשהשיחה מרגישה "תקועה"?

דמיינו שקיבלתם:

- "אני לא יכול לעזור בזה", או  
- תשובה ארוכה של שפה ביטחונית/מוסרנית שלא מתאימה למה שהתכוונתם.

במקום להילחם במערכת, אפשר לעבור ארבעה צעדים פשוטים:

1. **לשאול את עצמכם מה באמת רציתם** – ידע כללי? צעד פעולה? אבחנה?  
2. **לנקות מילים טעונות** – להחליף "אבחן" ב"נתח", "תן פרופיל" ב"תאר אפשרויות".  
3. **להצהיר על הכוונה** – "זה לצורכי לימוד בלבד, בלי ייעוץ אישי".  
4. **לפתוח שיחה חדשה** אם ההיסטוריה כבר "מזוהמת" בניסוחים בעייתיים.

---

### 7. תרגיל: ניסוח טעון מול ניסוח לימודי

1. בחרו נושא לגיטימי (למשל: למידה, ניהול זמן, שיתוף פעולה בצוות).  
2. כתבו פרומפט ראשון **טעון** – עם מילים שיכולות להישמע בעייתיות.  
3. ראו איך המודל עונה.  
4. כתבו פרומפט שני:
   - עם אותה כוונה כללית,  
   - אבל בניסוח לימודי/כללי/בטוח יותר.  
5. השוו בין התשובות:
   - איך השתנה הטון?  
   - איך השתנתה רמת השימושיות?  
   - איזה ניסוח מרגיש לכם מקצועי ואחראי יותר?

אפשר גם לבקש מה‑AI עצמו:

> "הנה ניסוח של שאלה ששאלתי ותשובה שקיבלתי.  
>  תנתח/י אילו מילים בפרומפט שלי עלולות להפעיל שכבות בטיחות,  
>  ותציע/י ניסוח מחדש בטוח ולימודי יותר,  
>  ששומר על אותה כוונה כללית."

המטרה היא לפתח **חוש לשפה**: להבין איך מילה אחת יכולה "לנעול" שיחה,  
ואיך ניסוח ברור ובטוח יכול "לפתוח" אותה – בלי לנסות לעקוף את כללי הבטיחות.


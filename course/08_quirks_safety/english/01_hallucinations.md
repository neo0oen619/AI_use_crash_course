## Hallucinations – when AI makes things up

### What is a hallucination?

A hallucination is when the AI gives an answer that:
- sounds confident, detailed, and well‑written,  
- but is simply **wrong** (factually or in essence).

Examples:
- inventing studies or articles that don’t exist,  
- giving wrong references or dates,  
- explaining a concept incorrectly.

### Why does this happen?

- The model doesn’t “look up the truth”; it generates plausible text.  
- When it lacks information or is uncertain, it may still produce a confident answer.  
- Its training data can be incomplete or inconsistent.

### What can you do?

- Ask it to **flag uncertainty**:

> “Answer, but also tell me where you are less confident and where I should check another source.”

- Treat references (studies, dates, exact numbers) as **suspect** until verified.  
- For important topics, always cross‑check with reliable sources (books, articles, professionals).

### Exercise

Ask the AI a factual question that you can verify (not critical, but checkable), then:

> “Rewrite your answer, and explicitly mark which parts might be wrong or need external verification.”

See whether it identifies some weak spots itself.


## Biases and limits – why AI is not “neutral”

### Biases (built‑in tilt)

The model was trained on texts written by humans.  
If those texts carry biases (about gender, culture, politics, etc.), the model can reflect them:
- some groups under‑represented,  
- some stereotypes repeated,  
- certain perspectives favored.

### Knowledge limits

- Training cuts off at a certain time (no continuous “live” learning).  
- New laws, research, or events might not be included.  
- Some domains are better covered than others.

### Edge cases

In very rare or complex situations:
- the model has fewer relevant examples from training,  
- answers become less reliable and more guess‑like.

### How to work with this

- Ask directly:

> “What kinds of biases could affect your answer to this question?”

- Ask for multiple perspectives, not just one:  
  “Give me 3 different viewpoints on this issue, and explain each briefly.”

Remember: the AI reflects patterns from its data; it is not a neutral judge.


## בטיחות, אימון וחורים בהתנהגות – למה לפעמים דברים מוזרים קורים

מודלי שפה גדולים עוברים:
- **אימון ראשוני** על כמויות ענק של טקסטים,  
- **כוונון (Fine-Tuning)** כדי להתאים למשימות מסוימות,  
- **אימון על משוב אנושי (RLHF)** כדי להפוך אותם ל”נעימים ובטוחים יותר”.

למרות זה, עדיין יש:
- אזורים שבהם הם חזקים,  
- ואזורי “צל” – מקומות עם חורים בהתנהגות ובבטיחות.

המטרה כאן היא לא להפחיד, אלא:
- לתת לכם **מודל מנטלי** של איפה כדאי להיות זהירים במיוחד,  
- ולהזכיר: אתם חלק ממנגנון הבטיחות – לא רק המודל.

---

### 1. שלושה שלבים (פשטניים) בתהליך האימון

1.**אימון בסיסי (Pretraining)**  
   - המודל “קורא” כמויות עצומות של טקסטים (אתרים, ספרים, מאמרים ועוד),  
   - ולומד משם בעיקר דבר אחד: **לחזות את הטוקן הבא** ברצף.  
   - בשלב הזה הוא עדיין לא “מנומס” או “בטוח” –  
     הוא פשוט מנסה לחקות דפוסים סטטיסטיים של שפה.

2.**כוונון למשימות (Supervised Fine-Tuning)**  
   - כאן כבר לא משתמשים “בכל הטקסטים שבעולם”,  
     אלא ב-*מערכי נתונים מסודרים* (Datasets) של דוגמאות.  
   - כל דוגמה נראית בערך כך:  
     **קלט:** שאלה / הנחיה / טקסט התחלתי  
     **פלט רצוי:** תשובה טובה שמישהו כתב מראש.  
   - המודל לומד:
     - איך נראית “תשובה טובה” בסגנון שאנחנו רוצים,  
     - איך להגיב היטב לסוגים מסוימים של בקשות (למשל: הסבר, סיכום, תרגום),  
     - איך להתאים את עצמו למשפחה מסוימת של משימות (לימוד, קוד, צ’אט כללי וכו’).

3.**אימון על משוב אנושי (RLHF)**  
   - בני אדם רואים כמה תשובות אפשריות של המודל לאותה בקשה,  
   - ומדרגים: מי מהן טובה יותר / ברורה יותר / בטוחה יותר.  
   - המודל לומד להעדיף:
     - תשובות מועילות,  
     - בטוחות וזהירות יותר,  
     - מנוסחות בצורה מנומסת ומכבדת.

עם זאת:

- אי אפשר ללמד אותו *כל מצב אפשרי*,  
- לא כל שילוב מוזר של הקשר, ניסוח, נושא ורגישות הופיע באימון,  
- ולכן נשארים אזורים שבהם ההתנהגות פחות צפויה –  
  ושם חשוב במיוחד שתישארו עם שיקול דעת ביקורתי משלכם.

אפשר לחשוב על זה כשני חלקים שעובדים יחד:

- **“המוח” של המודל** – מה שהוא למד מהטקסטים ומהדוגמאות (Pretraining + Fine-Tuning).  
- **“גדר הבטיחות”** – חוקים, פילטרים ומשוב אנושי שמנסים לעצור או לעדן תשובות בעייתיות.

חשוב לזכור:
- לפעמים “המוח” היה ממשיך לכיוון בעייתי, אבל גדר הבטיחות עוצרת אותו.  
- לפעמים להפך – גדר הבטיחות “ננעלת חזק מדי” וחוסמת גם תשובות לגיטימיות.  

ההפרדה הזו עוזרת להבין למה לפעמים מקבלים:
- תשובה זהירה ומוגבלת מדי,  
- או תשובה שנשמעת חכמה, אבל לא לוקחת בחשבון את כל הרגישויות.

---

### 2. איפה בדרך כלל הבטיחות חזקה יחסית?

בדרך כלל המודלים מיומנים היטב על:

-**נושאי סיכון ישירים**  
  - פגיעה עצמית,  
  - אלימות,  
  - הדרכה לעבירות פליליות,  
  - תכנים מיניים לא מתאימים.

-**טון בסיסי של זהירות**  
  - הדגשת הצורך להתייעץ עם אנשי מקצוע,  
  - הימנעות ממתן אבחנות רפואיות/פסיכולוגיות ישירות,  
  - שימוש בשפה יחסית מאוזנת.

במקומות האלה:
- לרוב תראו תשובות שמנסות להגן,  
- לפעמים אפילו יותר מדי (**Over-blocking**) –  
  כלומר, התשובה זהירה וחוסמת גם כשאתם מרגישים שלא היה בזה צורך.

---

### 3. איפה עלולים להופיע “חורים”?

כמה אזורים בעייתיים שבהם שווה להיות extra-ערניים:

-**שיחות מאוד ארוכות ומפותלות**  
  - קשה “לכסות” באימון כל רצף אפשרי של עשרות או מאות אלפי טוקנים,  
  - במצבים נדירים ושילובים מוזרים, ההתנהגות יכולה להיות פחות צפויה,  
  - במיוחד אם חלק מההקשר כבר נחתך (Truncation).

-**ניסוחים דו-משמעיים**  
  - שאלה שנשמעת תמימה, אבל יכולה להתפרש גם כבעייתית,  
  - המודל או שכבות הבטיחות יכולים לפעמים לפספס,  
  - או להפך – לחסום / להזהיר גם כשאין צורך.

-**נושאים רגישים אבל “אפורים”**  
  - למשל: דיבור מופשט על מצב נפשי,  
  - טיפים כלליים ללימוד/עבודה סביב נושאים רגישים,  
  - שאלות שמערבבות מחקר, טיפול ואנשים אמיתיים.

-**הטיות של נתוני האימון (Biases)**  
  - המודל למד מטקסטים שנכתבו על ידי בני אדם בעולם אמיתי,  
    ולכן הוא “יורש” גם חלק מההטיות שלהם:  
    לגבי מגדר, תרבות, מעמד, פוליטיקה ועוד.  
  - הוא יכול:
    - לתת דוגמאות שמייצגות בעיקר תרבות או קבוצה מסוימת,  
    - לפספס חוויות של קבוצות אחרות,  
    - או להציע ניסוחים שמרגישים פוגעניים/מקטינים לאנשים מסוימים.  
  - לכן, כשעובדים עם אוכלוסיות מגוונות –  
    חשוב לקרוא את התשובות בעין ביקורתית,  
    ולתקן ניסוחים/הנחות שלא מתאימים להקשר שלכם.

במקומות האלה, האחריות שלכם כמשתמשים חשובה במיוחד.

---

### 3.5 פרומפטים, “ג’יילברייק” וניסיונות לעקוף את המודל

במודולים אחרים דיברנו על **הנדסת פרומפטים (Prompt Engineering)** –  
היכולת לנסח בקשות בצורה מדויקת, כדי לקבל תשובות שימושיות יותר.

בגדול אפשר לחלק את זה לשני כיוונים:

-**שימוש חיובי** – לנסח ברור:
  - מי המודל (תפקיד),  
  - מה המשימה,  
  - מה הסגנון והמבנה הרצויים.  

-**שימוש התקפי / “עוקף”** – לנסות לבלבל או לעקוף את מנגנוני הבטיחות,  
  כדי לגרום למודל להחזיר תוכן שהוא *לא אמור* לתת (למשל: הדרכה לפעילות לא חוקית).

בקהילת ה-AI קוראים לזה לפעמים **Jailbreaking** – “פריצת כלא”:

- המודל מיועד לעבוד בתוך “גדר בטיחות”,  
- וג’יילברייק הוא ניסוח פרומפטים שמנסה לגרום לו “לשכוח” או לעקוף את הגדר הזו.

איך עושים את זה *ברמת רעיון* (בלי מתכונים מעשיים):

-משתמשים בהרבה **העמדת פנים**:
  - “תדמיין סיפור דמיוני שבו מישהו עושה X…”  
  - “תענה כאילו אתה דמות אחרת שלא כפופה לחוקים הרגילים…”  

-בונים **שכבות הוראות סותרות**:
  - למשל לבקש מהמודל “להתעלם מכללי בטיחות קודמים”  
    (למרות שבפועל המודלים החדשים בדרך כלל *לא* מצייתים לזה, וטוב שכך).

-משלבים **טקסטים ארוכים עם הוראות חבויות**  
  - למשל ניסיונות “להחביא” בקשה בעייתית בתוך מסמך תמים לכאורה.

-מנצלים **כלים חיצוניים**  
  - לפעמים דרך לינקים, קבצים או מערכות שמשלבות מודל שפה עם קוד/אינטרנט,  
  - ומנסים להחדיר לשם הוראות לא צפויות (Prompt Injection).

מנקודת המבט של הקורס הזה:

- אנחנו *לא* מתרגלים ג’יילברייקים,  
- *כן* חשוב להבין שהם קיימים –  
  כדי לא להניח ש”אם המודל הסכים אז כנראה שזה בטוח/חוקי”.

#### דוגמה מהעולם האמיתי: Pliny the Liberator

בשנים האחרונות הופיעה דמות אנונימית ברשת בשם **Pliny the Liberator**:

- “האקר” / חוקר עצמאי שמתמקד בג’יילברייק של מודלי שפה שונים.  
- מפרסם ברשתות חברתיות (X, דיסקורד, GitHub) דוגמאות לפרומפטים  
  שמצליחים לעקוף מנגנוני בטיחות של מודלים מסחריים וקוד פתוח.  
- בחלק מהכתבות מתארים אותו כמעין “פורץ כלא דיגיטלי” –  
  מי שמראה עד כמה קל לפעמים עדיין לגרום למודלים להפיק תוכן אסור או בעייתי.

יש על זה ויכוח ציבורי:

-מצד אחד – יש מי שרואים בזה **מחקר אבטחה**:
  - כמו “האקרים לבנים” שבודקים מערכות כדי לשפר אותן,  
  - חושפים חולשות כדי שחברות ה-AI יתקנו אותן.

-מצד שני – יש מי שחוששים ש:
  - שיתוף רחב של טכניקות כאלה יכול לעזור גם לשחקנים זדוניים,  
  - ויכול לעודד שימוש לרעה במודלים.

מה שחשוב לנו כמשתמשים:

- לדעת שהגדרות הבטיחות **לא מושלמות**,  
- להבין שיש אנשים שמשקיעים המון מאמץ בלעקוף אותן,  
- ולזכור שגם אם מישהו “הצליח להוציא מהמודל משהו שאסור” –  
  זה לא אומר שזה חכם, חוקי או מוסרי להשתמש בזה.

במסגרת הקורס הזה אנחנו מתייחסים לג’יילברייקים כאל *תופעה שצריך להכיר*  
– כדי להבין את הגבולות והחולשות של המערכות –  
אבל לא כמשהו ששואפים ליישם בעבודה היומיומית.

---

### 4. מה המשמעות עבור לומדים וחוקרים?

עבור מי שעובד/ת עם אנשים אמיתיים (פסיכולוגיה, טיפול, מחקר, חינוך וכו’):

-**אסור להתייחס ל-AI כמחליף שיקול דעת מקצועי**  
  - הוא יכול להציע רעיונות, הסברים, דוגמאות,  
  - אבל אין לו:
    - ראיון קליני,  
    - שיחה אמיתית,  
    - ידע על ההיסטוריה האישית של אדם מסוים.

-**חשוב להפריד בין “חומר רקע” לבין “הנחיות טיפול”**  
  - אפשר לשאול אותו על מודלים תיאורטיים, מחקרים, מושגים,  
  - אבל לא לבנות עליו כדי לקבל החלטות טיפוליות עבור אדם אמיתי.

-**במחקר**:  
  - אפשר להיעזר בו בניסוח שאלות, סיכום טקסטים, רעיונות לניתוח,  
  - אבל לא לתת לו “לנתח לבד” נתונים רגישים בלי הבנה של ההשלכות.

בנוסף, גם מבחינת **פרטיות** ו**אתיקה** חשוב לשאול:

- האם מותר לי בכלל להכניס את המידע הזה לצ’אט?  
- האם יש כאן פרטים מזהים על אדם אמיתי, סטודנט, מטופל, קולגה?  
- האם אני שומר/ת על חיסיון, על כללי אתיקה מקצועיים, ועל מה שהובטח לאנשים במפורש?

המודל לא יודע לבד מה מותר ומה אסור לכם לשתף –  
זו אחריות שלכם.

מודולים 07–08 מרחיבים את זה לכיוון מוזרויות, אתיקה ופרטיות,  
ונותנים דוגמאות נוספות למצבים גבוליים.

---

### 5. מה לעשות כשמשהו מרגיש מוזר?

אם קיבלתם תשובה שמרגישה לא נכונה, קיצונית או פשוט “לא יושבת טוב בבטן”:

1.**לעצור – לא לרוץ ישר לפי התשובה**  
   - במיוחד אם זה קשור לבריאות, טיפול, יחסים, כסף או אנשים אמיתיים.

2.**לשאול את המודל על המגבלות שלו**  
   > “על מה הסתמכת בתשובה הזו?”  
   > “איפה יכול להיות שטעית כאן?”  
   > “איזה סיכונים אתה רואה בגישה שהצעת?”

3.**לנסח מחדש את השאלה**  
   - להיות יותר ספציפיים,  
   - לבקש גם חסרונות, סיכונים והנחות יסוד,  
   - להבהיר שלא מחפשים “תשובה אחת נכונה וסופית”.

4.**להצליב עם מקור נוסף**  
   - ספר / מאמר / קולגה / איש מקצוע,  
   - או לפחות מודל אחר / שיחה אחרת.

5.**בנושאים רגישים במיוחד – לדבר עם אדם אמיתי**  
   - במיוחד בטיפול, חינוך, מצוקה, החלטות גדולות.  

הרעיון:  
ה-AI הוא שותף לחשיבה –  
אבל *אתם* אלה שלוחצים על כפתור “אישור סופי”.

---

### 6. תרגיל – לבנות “הצהרת בטיחות אישית”

אחרי קריאת הפרק הזה, נסו לנסח לעצמכם 5–7 נקודות:

> “איך *אני* רוצה להשתמש ב-AI בצורה אחראית?”

לדוגמה:
- “לא משתמש ב-AI למתן אבחנות או עצות טיפוליות.”  
- “בודק כל טענה עובדתית חשובה מול לפחות מקור נוסף אחד.”  
- “משתמש ב-AI כדי לסדר מחשבות, לא כדי להחליט במקום.”  
- “אם התשובה נוגעת לנושא רגיש – עוצר, חושב, מתייעץ.”  
- “לא מכניס לצ’אט פרטים מזהים על אנשים בלי לוודא שזה מותר ומתאים.”  

אפשר גם לכתוב ל-AI:

> “אני רוצה לבנות לעצמי 7 כללי אצבע לשימוש אחראי ב-AI  
> כאדם שעוסק ב[מחקר / הוראה / טיפול / תחום אחר].  
> תציע/י רשימה ראשונית,  
> ואז תשאל/י אותי כמה שאלות כדי להתאים אותה אליי באופן אישי.”

אחרי שגיבשתם “הצהרת בטיחות אישית”:

- אפשר לשמור אותה בתחילת המחברת או בקובץ קבוע,  
- ואפשר גם להשתמש בה כחלק מהודעת הפתיחה לצ’אט:

> “לפני שנתחיל, חשוב לי שתזכור:  
>  אני משתמש/ת בך רק כחלק מתהליך חשיבה,  
>  לא מקבל/ת ממך אבחנות או החלטות טיפוליות,  
>  ותמיד בודק/ת עובדות חשובות מול מקור נוסף.”

כך ההצהרה שלכם הופכת גם לכלל פנימי *וגם* לפרומפט חיצוני –  
ומחזקת את הסיכוי שתשתמשו ב-AI בצורה אחראית לאורך זמן.

## איך מודלי שפה גדולים “חושבים” – אינטואיציה בלי נוסחאות

בפרק הזה ננסה להבין, ברמה אינטואיטיבית, מה קורה בתוך מודל שפה גדול (LLM).  
הכוונה אינה ללמד מתמטיקה, אלא לתת לכם **דימוי עבודה** שיעזור להבין:
- למה הוא עונה כפי שהוא עונה,  
- למה הוא לפעמים מדויק, ולפעמים “ממציא”,  
- ואיך זה קשור לאופן שבו אתם כותבים פרומפט.

---

### 1. הניסוי המחשבתי: “להשלים משפטים”

דמיינו שאתם קוראים הרבה מאוד טקסטים,  
ובכל פעם שמופיעה התחלה של משפט, שואלים אתכם:

> “מה המילה/המשך הכי סביר שיבוא עכשיו?”

אחרי שנים של תרגול, תתחילו:
- לזהות תבניות שפה,  
- לדעת אילו מילים הולכות יחד,  
- לשער מה ייכתב אחרי ניסוח מסוים.

מודל שפה גדול עושה משהו דומה – רק:
- על כמויות טקסט שאין סיכוי שאדם אחד יקרא,  
- באמצעות רשת עצומה של “משקלים” (פרמטרים) במקום מוח ביולוגי.

העיקרון הבסיסי:

> המודל לא “מבין את האמת”,  
> הוא **מנחש טוקן הבא** על בסיס דפוסים שראה בעבר.

לפעמים הניחוש הזה מדויק ומועיל,  
ולפעמים הוא נשמע טוב – אבל מוטעה (הזיה).

---

### 2. המבנה הגס: קלט → המרה לטוקנים → שכבות → פלט

בצורה מאוד סכמטית, אפשר לתאר את העבודה של LLM ככה:

1.**קלט (Input)**  
   אתם כותבים טקסט: הוראות, שאלות, קטעים להמשך, מסמכים.

2.**פירוק לטוקנים**  
   הטקסט נשבר ליחידות קטנות (Tokens) – חלקי מילים, מילים, סימני פיסוק.

3.**מעבר דרך שכבות**  
   כל רצף הטוקנים עובר דרך הרבה שכבות של חישוב (רשת נוירונים עמוקה),  
   שבכל שכבה “לומדת” יחס בין הטוקנים – מה קשור למה, מה חשוב יותר וכו’.

4.**בחירת הטוקן הבא**  
   בסוף, המודל מחשב התפלגות הסתברות:  
   מה הסיכוי לכל טוקן להיות הבא בתור.  
   על פי הגדרות כמו Temperature, הוא בוחר טוקן אחד,  
   מוסיף אותו לטקסט – וממשיך לנבא את הטוקן הבא.

התוצאה:
- נוצר רצף מילים שנראה כמו שיחה,  
- כאילו יש שם “קו מחשבה”,  
- אבל בפועל זה רצף של ניחושים הסתברותיים.

---

### 3. “אבל זה מרגיש שהוא מבין…” – למה?

יש כמה סיבות טובות לכך שהתשובות של המודל *מרגישות* מלאות הבנה:

-**היקף החומר**  
  הוא נחשף (אימונית) להרבה מאוד טקסטים: מאמרים, ספרים, אתרים, שיחות.  
  לכן יש לו דוגמאות מוכנות כמעט לכל צורת ניסוח.

-**שפה טבעית זורמת**  
  הוא יודע לייצר טקסט:
  - עם פתיחה וסגירה,  
  - עם כותרות,  
  - עם רשימות וטבלאות,  
  - בסגנונות שונים (רשמי, חברי, אקדמי).

-**יכולת לחקות תפקידים (Personas)**  
  אם מבקשים ממנו “תהיה מורה”, “תהיה עורך”, “תהיה שותף מחקר” –  
  הוא לוקח דפוסים של טקסטים שבהם דיברו כך, ומחקה אותם.

כול זה יוצר תחושת “אדם מצדו השני של הצ’אט”.  
אבל מתחת – זה עדיין מנגנון של ניבוי טקסט על בסיס דפוסים.

---

### 4. השפעת Temperature וקצת על “מצבי רוח”

כאשר קוראים על מודלים, נתקלים לעיתים בפרמטרים כמו:
- ה **Temperature**,  
- ה **Top‑p**,  
- ה **Top‑k**.

בלי להיכנס לנוסחאות:

-ה Temperature נמוך (למשל 0–0.3):
  - המודל בוחר כמעט תמיד את הטוקן הכי סביר,  
  - התשובות יציבות וחוזרות על עצמן יותר,  
  - יש פחות יצירתיות, אבל גם פחות “קפיצות מוזרות”.

-ה Temperature גבוה (למשל 0.8–1.0):
  - המודל נותן יותר מקום לטוקנים “פחות צפויים”,  
  - התשובות יכולות להיות יצירתיות/מפתיעות,  
  - אבל גם לפעמים מנותקות או מבולבלות.

אפשר לחשוב על זה כמו:
- מצב “זהיר ושמרן” מול מצב “יצירתי ומשוחרר יותר”.

חשוב:
- גם ב‑Temperature נמוך, עדיין יש סיכוי להזיות,  
- גם ב‑Temperature גבוה, אפשר לקבל תשובות מעולות – אם השאלה טובה.

---

### 5. למה חשוב להבין את זה כמשתמשים?

הבנת מנגנון העבודה (ברמה הזו) עוזרת לכם:

-**להבין שהשפה משפיעה מאוד**  
  – שינוי קטן בניסוח יכול להזיז את כל ההתפלגות של הטוקן הבא,  
  – ולכן פרומפטים טובים הם תוצאה של ניסוי ושיפור, לא רק “שאלה אחת”.

-**להבין שהמודל לא “מתעקש על האמת”**  
  – הוא לא עוצר כדי לשאול את עצמו “האם זה נכון?”,  
  – אלא “האם זה המשך סביר לדפוסי טקסט שראיתי?”.  
  לכן יש צורך בשכבת בדיקה אנושית.

-**להבין למה שיחות ארוכות מתנהגות מוזר**  
  – כי חלון הקשר מוגבל,  
  – כי חיתוך (Truncation) משנה את מה שהמודל “רואה”,  
  – כי דפוסים שנוצרו מוקדם בשיחה ממשיכים להשפיע על ניסוחים מאוחרים.

במודולים הבאים נעמיק בטוקנים, הקשר וחיתוך (02–03),  
ונלמד איך להנדס פרומפטים שלא “מבלבלים” את המודל.

---

### תרגיל: לדבר עם המודל על איך הוא עובד

נסו לפתוח צ’אט חדש ולכתוב משהו בסגנון:

> “תסביר/י לי איך אתה/את עובד/ת בתור מודל שפה גדול,  
> בלי נוסחאות, במקסימום 10 משפטים.  
> אחר‑כך:
> 1. תן/י דוגמה אחת לשאלה שבה אתה נוטה לטעות,  
> 2. תסביר/י למה את/ה טועה שם לעיתים,  
> 3. תציע/י מה אני יכול/ה לעשות בפרומפט  
>    כדי להקטין את הסיכוי לטעות כזו.”

השוו בין ההסבר של המודל לבין ההסבר שקראתם כאן.  
איפה הם דומים? איפה הם שונים?  
איזו גרסה עוזרת לכם יותר להבין איך לעבוד איתו בחיי היום‑יום?

